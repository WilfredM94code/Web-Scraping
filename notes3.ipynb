{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In web scraping there are several data associated with a website. Data is\n",
    "categorized as either dynamic or static. Dynamic parsing is another \n",
    "discipline of webscraping.\n",
    "\n",
    "Static parsing interacts with HTML/CSS files\n",
    "Dynamic parsing parses Documents-Object-Model objects\n",
    "\n",
    "There are also web scraping using Computer Vision using a tool called Sikuli \n",
    "which reaches visible data from an interface\n",
    "\n",
    "For static parsing is commonly used Beautiful Soup\n",
    "For dynamic parsing is commonly used Scrapy\n",
    "\n",
    "Among the data extraction from the web (commonly called web scraping) there's \n",
    "also another activity called web crawling which is finding or discovering \n",
    "URLs or links on the web which requires to download and index websites. This is\n",
    "an activity performed by all search engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy is a framework, and the difference between Library and Framework is that\n",
    "a library offers a set of functionallities to add to a code, while a framework\n",
    "is a system that requires user to set it up to adapt multiple already defined\n",
    "functionallities\n",
    "\n",
    "Just like BS, Scrapy requires to know what to look for which requires a set of \n",
    "rules to be established to work and is strictly tied to the way a website\n",
    "is strictured. If website changes, code must change accordingly. \n",
    "\n",
    "With Scrapy data can be accessed using CSS and XPath expressions. For this, Scrapy\n",
    "uses Selectors.\n",
    "\n",
    "Scrapy can be used in a terminal or can be leveraged with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy Shell\n",
    "Once Scrapy is installed (Using either 'pip install scrapy' or 'conda install scrapy')\n",
    "It can be accessed using the 'scrapy' command from terminal returning the following\n",
    "text:\n",
    "\n",
    "###\n",
    "Scrapy 2.6.2 - no active project\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  bench         Run quick benchmark test\n",
    "  commands      \n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "  genspider     Generate new spider using pre-defined templates\n",
    "  runspider     Run a self-contained spider (without creating a project)\n",
    "  settings      Get settings values\n",
    "  shell         Interactive scraping console\n",
    "  startproject  Create new project\n",
    "  version       Print Scrapy version\n",
    "  view          Open URL in browser, as seen by Scrapy\n",
    "\n",
    "  [ more ]      More commands available when run from project directory\n",
    "\n",
    "Use \"scrapy <command> -h\" to see more info about a command\n",
    "###\n",
    "\n",
    "Also the 'scrapy version' provides the (you guessed) Scrapy version\n",
    "###\n",
    "Scrapy 2.6.2\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Scrapy has an integrated Benchmark module to test the computer's capacity to\n",
    "web crawl over local a test environment. This feature is accessible by using the \n",
    "'scrapy bench' command. Prompt will return a message like the following\n",
    "\n",
    "###\n",
    "2022-08-06 16:57:01 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
    "2022-08-06 16:57:01 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ], pyOpenSSL 22.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 37.0.1, Platform macOS-10.16-x86_64-i386-64bit\n",
    "2022-08-06 16:57:02 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'CLOSESPIDER_TIMEOUT': 10, 'LOGSTATS_INTERVAL': 1, 'LOG_LEVEL': 'INFO'}\n",
    "2022-08-06 16:57:02 [scrapy.extensions.telnet] INFO: Telnet Password: 70427ee0f7bcfd37\n",
    "2022-08-06 16:57:02 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage',\n",
    " 'scrapy.extensions.closespider.CloseSpider',\n",
    " 'scrapy.extensions.logstats.LogStats']\n",
    "2022-08-06 16:57:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2022-08-06 16:57:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2022-08-06 16:57:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2022-08-06 16:57:02 [scrapy.core.engine] INFO: Spider opened\n",
    "2022-08-06 16:57:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2022-08-06 16:57:03 [scrapy.extensions.logstats] INFO: Crawled 53 pages (at 3180 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:04 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:05 [scrapy.extensions.logstats] INFO: Crawled 149 pages (at 2400 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:06 [scrapy.extensions.logstats] INFO: Crawled 189 pages (at 2400 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:08 [scrapy.extensions.logstats] INFO: Crawled 229 pages (at 2400 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:08 [scrapy.extensions.logstats] INFO: Crawled 245 pages (at 960 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:09 [scrapy.extensions.logstats] INFO: Crawled 293 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:10 [scrapy.extensions.logstats] INFO: Crawled 341 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:11 [scrapy.extensions.logstats] INFO: Crawled 389 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:12 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)\n",
    "2022-08-06 16:57:12 [scrapy.extensions.logstats] INFO: Crawled 437 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n",
    "2022-08-06 16:57:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 187218,\n",
    " 'downloader/request_count': 453,\n",
    " 'downloader/request_method_count/GET': 453,\n",
    " 'downloader/response_bytes': 1222431,\n",
    " 'downloader/response_count': 453,\n",
    " 'downloader/response_status_count/200': 453,\n",
    " 'elapsed_time_seconds': 10.695523,\n",
    " 'finish_reason': 'closespider_timeout',\n",
    " 'finish_time': datetime.datetime(2022, 8, 6, 20, 57, 13, 381519),\n",
    " 'log_count/INFO': 20,\n",
    " 'memusage/max': 60149760,\n",
    " 'memusage/startup': 60149760,\n",
    " 'request_depth_max': 16,\n",
    " 'response_received_count': 453,\n",
    " 'scheduler/dequeued': 453,\n",
    " 'scheduler/dequeued/memory': 453,\n",
    " 'scheduler/enqueued': 9061,\n",
    " 'scheduler/enqueued/memory': 9061,\n",
    " 'start_time': datetime.datetime(2022, 8, 6, 20, 57, 2, 685996)}\n",
    "2022-08-06 16:57:13 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch the content from a website there can be used the 'scrapy fetch [URLHERE]'.\n",
    "This command will return the HTML conetent. The web content will be displayed.\n",
    "To store it there has to be provided the following command \n",
    "'scrapy fetch [URLHERE] > [PATH/FILENAME.html]'. If only filename provided the \n",
    "HTML file will be stored in the cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open the browser with Scrapy there can be used the command \n",
    "'scrapy view [URLHERE]'. Note that this command will use the default web browser \n",
    "to open an locally stored temporal version of such website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy interactive shell allows to interact with multiple functionallities over\n",
    "a website. This shell instatiates multiple objects that allows to get data \n",
    "from a website. To access scrapy shell there has to be used the following command:\n",
    "'scrapy shell [URLHERE]'. Once executed this command scrapy will display all \n",
    "available objects. To get in clean the list of objects there can be used the\n",
    "'shelp()' command once the shell is open. The prompt returns the following\n",
    "message:\n",
    "###\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7fc95030f0a0>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://stackoverflow.com/>\n",
    "[s]   response   <200 https://stackoverflow.com/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7fc95030f880>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7fc950743640>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "###\n",
    "\n",
    "- spider: Contains the definitions and details onto how data will be gathered. In this\n",
    "case scrapy has a 'default' spider\n",
    "\n",
    "- scrapy: Provides the module information. It provides the selectors, request \n",
    "instructions\n",
    "\n",
    "- crawler: API that provides access to scrapy components\n",
    "\n",
    "- item: Container for structured data extracted from a webpage\n",
    "\n",
    "- request: Gives access to the web request object\n",
    "\n",
    "- response: Gives access to the web response object\n",
    "\n",
    "The 'view()' function recieves a response object to accesss the locally stored web\n",
    "page. Note that this is am static view, no dynamic content will be updated/displayed.\n",
    "\n",
    "The scraping process can be done over local HTML files\n",
    "\n",
    "Everytime the 'fetch' command is executed with different URL scrapy will work over\n",
    "that new URL and every scrapy object will be updated to work with this URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get data from a website there can be used CSS selectors. For this example there\n",
    "will be used the following website\n",
    "\n",
    "https://www.w3schools.com/howto/howto_website_static.asp\n",
    "\n",
    "This web page will be accessed by executing the following command\n",
    "\n",
    "'scrapy shell https://www.w3schools.com/howto/howto_website_static.asp'\n",
    "\n",
    "Once executed scrapy object will be available \n",
    "\n",
    "To select an item using a css element there has to be executed the following \n",
    "command\n",
    "\n",
    "'response.css('[CSS]')'\n",
    "\n",
    "In this case there will be used the argument 'title'\n",
    "\n",
    "'response.css('title')'\n",
    "\n",
    "Prompt will return the following \n",
    "###\n",
    "[<Selector xpath='descendant-or-self::title' data='<title>How To Make a Static Website</...'>]\n",
    "###\n",
    "\n",
    "This method will return a <class 'scrapy.selector.unified.SelectorList'>\n",
    "Such object contains multiple methods. The 'get()' allows to get the HTML tag with\n",
    "its content\n",
    "\n",
    "Note that such command will return every tag matching the tag name\n",
    "Every method run over this object/list of objects will be executed over the 1st \n",
    "item.\n",
    "\n",
    "To get every tag there can be used the 'getall()' method. This method returns \n",
    "every tag content ina list\n",
    "\n",
    "The 'extract()' method returns exactly the same list as the 'getall()' method.\n",
    "\n",
    "The 'extractfirst()' method returns the content of the 1st element\n",
    "\n",
    "This can be used to select tag attributes\n",
    "\n",
    "'response.css('img::attr(src)').extract()'\n",
    "\n",
    "To get the attribute from a selected tag there can be used the attribute name as a\n",
    "keyword to call such attribute value\n",
    "\n",
    "'response.css('img').attrib['src']'\n",
    "\n",
    "This object can be indexed and each method can be executed upon each\n",
    "element\n",
    "\n",
    "Note that a particular tag can be selected by using the full CSS path/address\n",
    "\n",
    "'response.css('.w3-main > .w3-row > .w3-col > .w3-info > .ws-btn ::text ')'\n",
    "\n",
    "There can be selected elements by using only a class\n",
    "\n",
    "response.css('.ws-btn::text')\n",
    "\n",
    "A selection can be made by \n",
    "\n",
    "'response.css('.w3-info a::text')'\n",
    "\n",
    "The CSS selector can be accessed by using a web-browser such as Chrome, get into \n",
    "Inspect menu and then, over a tag, get on 'copy' menu and select 'Copy selector'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get data from a website there can be used XPath selectors. For this example there\n",
    "will be used the following website\n",
    "\n",
    "The XPath can be accessed by using a web-browser such as Chrome, get into Inspect \n",
    "menu and then, over a tag, get on 'copy' menu and select 'Copy XPath' or \n",
    "'Copy full XPath'. There are absolute XPaths and relative XPaths (with '//' at the \n",
    "beginning of the addres)\n",
    "\n",
    "Relative XPath example:\n",
    "'//*[@id=\"jsid-post-a61PLXq\"]/div[1]/div/a/div/video'\n",
    "\n",
    "Absolute XPath example:\n",
    "'/html/body/div[1]/div/div[1]/div/div[1]/section/div[1]/article[1]/div[1]/div/a/div/video'\n",
    "\n",
    "To select elements using XPaths there can be used the following command\n",
    "\n",
    "'response.xpath([XPATHADRRESS])'\n",
    "\n",
    "XPath addres can point to the tag content of the XPath as text\n",
    "\n",
    "'response.xpath('[XPATHADRRESS]/text()')'\n",
    "\n",
    "'response.xpath('//*[@id=\"main\"]/h1/text()').get()'\n",
    "\n",
    "This method also works with full XPaths\n",
    "\n",
    "'response.xpath('/html/body/div[7]/div[1]/div[1]/h1/text()').get()'\n",
    "\n",
    "To get attributes those have to be preceded by an '@' \n",
    "\n",
    "response.xpath('[XPATHADRRESS]@[ATTRIBUTE]')\n",
    "\n",
    "'response.xpath('//*[@id=\"main\"]/div[7]/div/a/img/@src').get()'\n",
    "\n",
    "This selector also works with the 'extract()', 'extract_first()', 'get()',\n",
    "'get_all()'  and the attribute selection by keywonds with the 'attrib' attribute\n",
    "\n",
    "To make more complex element selectors there can be used the '*' character to \n",
    "denotate the flexibility over a XPath selector and select tags over a pattern.\n",
    "Relative XPath gets such character by default, and it can be used to contruct a \n",
    "custom XPath\n",
    "\n",
    "'response.xpath('//*/img/@src').extract()'\n",
    "\n",
    "This can also be used to select multiple objects by adding a conditional within\n",
    "[]\n",
    "\n",
    "'response.xpath('//*[contains(text(), \"Template\")]')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy works with objects/classes called 'Spider'. This can be used by default\n",
    "or written by the user. This spiders define how web data is parsed, how items \n",
    "are selected and extracted.\n",
    "\n",
    "Scrapy contains an engine that allows to control an Scheduler. This Scheduler \n",
    "manages requests and responses. Every request is then processed by a designated\n",
    "spider. Each request recieves data by a Downloader that then serves data to the\n",
    "engine. \n",
    "\n",
    "Data gathered from spiders can then passed to a data pipeline for validation, \n",
    "cleaning, processing, etc.\n",
    "\n",
    "Each spider must have defined the\n",
    "1 - What to crawl. URL to start\n",
    "2 - How to crawl. Callback function inputs web pages and output items, requests,\n",
    "etc.\n",
    "3 - How to parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined the spider behaviour there can be created a scrapy project.\n",
    "\n",
    "To do so there has to be a terminal window open. It is recommended to designate\n",
    "a directory for the project alone.\n",
    "\n",
    "To start the project there has to be executed the following command\n",
    "\n",
    "'scrapy startproject [NAMEFORTHEPROJECT]'\n",
    "\n",
    "This command created a config file and a directory for the spider. Inside the \n",
    "spider directory there will be a set of default files corresponding to the spider\n",
    "\n",
    "'__init__.py     middlewares.py  settings.py\n",
    "items.py        pipelines.py    spiders'\n",
    "\n",
    "This default class is meant to be instantiated. To do so there has to be executed\n",
    "the following command\n",
    "\n",
    "'cd simplespider [NAMEFORTHEPROJECT]\n",
    "scrapy genspider [NAMEFORTHESPIDERINSTANCE] [URL]'\n",
    "\n",
    "Prompt will return the follwing message\n",
    "\n",
    "Created spider '[NAMEFORTHESPIDERINSTANCE]' using template 'basic' in module:\n",
    "  stockdata.spiders.details\n",
    "\n",
    "To set up a spider there has to be oppened the [NAMEFORTHESPIDERINSTANCE].py file\n",
    "recently created in a text editor. In this case the spider instance was called\n",
    "'details'.\n",
    "\n",
    "The contetn of the file is as follows:\n",
    "\n",
    "'import scrapy\n",
    "\n",
    "\n",
    "class DetailsSpider(scrapy.Spider):\n",
    "    name = 'details'\n",
    "    allowed_domains = ['finance.yahoo.com']\n",
    "    start_urls = ['http://finance.yahoo.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass'\n",
    "\n",
    "The 'start_urls' list will be modified to have only the element \n",
    "'https://finance.yahoo.com/trending-tickers'\n",
    "\n",
    "The parse method requires to be coded to provide a criteria to select elements\n",
    "for didactic porpuses there will be modified\n",
    "\n",
    "def parse(self, response):\n",
    "    xpath_a = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr[1]/td[2]/text()'\n",
    "    xpath_b = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr[1]/td[3]/fin-streamer/text()'\n",
    "    company_name_list = response.xpath(xpath_a).extract()\n",
    "    company_price_list = response.xpath(xpath_b).extract()\n",
    "    for name,price in zip(company_name_list,company_price_list):\n",
    "        print (f'{name} - {price}')\n",
    "\n",
    "Once saved this changes an spider can be executed by using the following command\n",
    "\n",
    "(from project folder)\n",
    "'scrapy crawl details'\n",
    "\n",
    "Once executed there will be a extense (long ass) prompt output. This spider was \n",
    "designed to get company names and stock prices\n",
    "\n",
    "Lulu's Fashion Lounge Holdings, Inc. - 5.71\n",
    "Schwab U.S. Dividend Equity ETF - 74.61\n",
    "Signify Health, Inc. - 19.87\n",
    "Loopring USD - 0.483408\n",
    "Avalanche USD - 28.05\n",
    "Gold Royalty Corp. - 2.9200\n",
    "Nkarta, Inc. - 14.89\n",
    "Bed Bath & Beyond Inc. - 8.16\n",
    "Lululemon Athletica Inc. - 317.80\n",
    "Air Products and Chemicals, Inc. - 262.64\n",
    "Zoom Video Communications, Inc. - 113.85\n",
    "Novavax, Inc. - 60.27\n",
    "Bonso Electronics International Inc. - 4.7800\n",
    "Netflix, Inc. - 226.78\n",
    "Roku, Inc. - 82.26\n",
    "Palantir Technologies Inc. - 11.45\n",
    "Polkadot USD - 8.7659\n",
    "Stanley Black & Decker, Inc. - 95.97\n",
    "NIO Inc. - 20.22\n",
    "ChargePoint Holdings, Inc. - 16.06\n",
    "Crypto.com Coin USD - 0.147405\n",
    "Plug Power Inc. - 25.19\n",
    "Sunrun Inc. - 33.24\n",
    "Algorand USD - 0.359461\n",
    "CVS Health Corporation - 102.26\n",
    "Etsy, Inc. - 109.38\n",
    "Natural Gas Sep 22 - 8.005\n",
    "Digital Turbine, Inc. - 22.88\n",
    "Shopify Inc. - 40.81\n",
    "SIGA Technologies, Inc. - 23.30\n",
    "2022-08-07 15:33:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2022-08-07 15:33:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 468,\n",
    " 'downloader/request_count': 2,\n",
    " 'downloader/request_method_count/GET': 2,\n",
    " 'downloader/response_bytes': 193667,\n",
    " 'downloader/response_count': 2,\n",
    " 'downloader/response_status_count/200': 1,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'elapsed_time_seconds': 65.981439,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2022, 8, 7, 19, 33, 0, 679806),\n",
    " 'httpcompression/response_bytes': 932566,\n",
    " 'httpcompression/response_count': 2,\n",
    " 'log_count/DEBUG': 35,\n",
    " 'log_count/INFO': 11,\n",
    " 'memusage/max': 60891136,\n",
    " 'memusage/startup': 58998784,\n",
    " 'response_received_count': 2,\n",
    " 'robotstxt/request_count': 1,\n",
    " 'robotstxt/response_count': 1,\n",
    " 'robotstxt/response_status_count/404': 1,\n",
    " 'scheduler/dequeued': 1,\n",
    " 'scheduler/dequeued/memory': 1,\n",
    " 'scheduler/enqueued': 1,\n",
    " 'scheduler/enqueued/memory': 1,\n",
    " 'start_time': datetime.datetime(2022, 8, 7, 19, 31, 54, 698367)}\n",
    "2022-08-07 15:33:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "\n",
    "Note that there's a set of key and values between '{}' available at the end of the\n",
    "output. That is the spider activity performance results. These are meant to be used\n",
    "for the debugging process.\n",
    "\n",
    "To create more spiders there can be stored in the following folder\n",
    "\n",
    "'[NAMEFORTHEPROJECT]/[NAMEFORTHEPROJECT]/spiders'\n",
    "\n",
    "Exactly where the 'details.py' file is stored. This additional spider will get the \n",
    "same data and store it in a simple txt file. This new spider is called 'details2.py'\n",
    "\n",
    "Note that there were no path added to the filename, which means that Scrapy will\n",
    "save the info in a file in the CWD where is executed. That means that data will be\n",
    "stored in the project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get data from a webpage there has to be a basic exploration of the \n",
    "data available. There's a class from the package Scrapy called 'Item' that is\n",
    "used for such task. This process can be leveraged by using a text editor such as\n",
    "VSCode to code the following, but the code is meant to be executed in the \n",
    "scrapy shell over the 'https://finance.yahoo.com/trending-tickers' website.\n",
    "\n",
    "There will be created a class following the next structure\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class DetailItem(scrapy.Item):\n",
    "    company_name = scrapy.Field()\n",
    "    company_price = scrapy.Field()\n",
    "    company_symbol = scrapy.Field()\n",
    "    company_link = scrapy.Field()\n",
    "\n",
    "details_item = DetailItem()\n",
    "\n",
    "This class inherits methods from the 'scrapy.Item' class. It declres a set of \n",
    "variables which are instances of the 'scrapy.Field' class. Once this class is \n",
    "defined it will be intatiated. This resulting object can store in each 'scrapy.Field'\n",
    "instance a selector as shwon below.\n",
    "\n",
    "'# Instance\n",
    "details_item = DetailItem()\n",
    "\n",
    "# XPaths\n",
    "xpath_a = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[1]/text()'\n",
    "xpath_b = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[2]/text()'\n",
    "xpath_c = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[3]/fin-streamer/text()'\n",
    "xpath_d = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[1]/a/text()'\n",
    "\n",
    "# Keyword assignment\n",
    "details_item['company_symbol'] = response.xpath(xpath_a).extract()\n",
    "details_item['company_name'] = response.xpath(xpath_b).extract()\n",
    "details_item['company_price'] = response.xpath(xpath_c).extract()\n",
    "details_item['company_link'] = response.xpath(xpath_d).extract()'\n",
    "\n",
    "Once these selectors are tested and proved to actually store data this class can be\n",
    "placed in the 'items.py' file within the following relative dir\n",
    "\n",
    "'[NAMEFORTHEPROJECT]/[NAMEFORTHEPROJECT]'\n",
    "\n",
    "'class DetailItem(scrapy.Item):\n",
    "    company_symbol = scrapy.Field()\n",
    "    company_name = scrapy.Field()\n",
    "    company_price = scrapy.Field()\n",
    "    company_link = scrapy.Field()'\n",
    "\n",
    "For didactic porpuses there will be created a 'details3.py' spider.\n",
    "In this file there will stored the following code\n",
    "\n",
    "import scrapy\n",
    "from simplespider.items import DetailItem\n",
    "\n",
    "'class ItemData(scrapy.Spider):\n",
    "    name = 'itemdata'\n",
    "    allowed_domains = ['finance.yahoo.com']\n",
    "    start_urls = ['https://finance.yahoo.com/trending-tickers']\n",
    "    def parse(self,response):\n",
    "        # XPaths\n",
    "        xpath_a = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[1]/a/text()'\n",
    "        xpath_b = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[2]/text()'\n",
    "        xpath_c = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[3]/fin-streamer/text()'\n",
    "        xpath_d = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr/td[1]/a/@href'\n",
    "        # XPath parse assignment\n",
    "        company_symbol = response.xpath(xpath_a).extract()\n",
    "        company_name = response.xpath(xpath_b).extract()\n",
    "        company_price = response.xpath(xpath_c).extract()\n",
    "        company_link = response.xpath(xpath_d).extract()\n",
    "        # DetailItem instance \n",
    "        details_item = DetailItem()\n",
    "        # Loop assignment\n",
    "        for symbol, name, price, link in zip(company_symbol, company_name, company_price, company_link):\n",
    "            details_item['company_symbol'] = symbol\n",
    "            details_item['company_name'] = name\n",
    "            details_item['company_price'] = price\n",
    "            details_item['company_link'] = link\n",
    "            yield details_item'\n",
    "\n",
    "To get such information available in a file there can be used the following flag when\n",
    "executing a spider\n",
    "\n",
    "scrapy crawl [SPIDERNAME] -o [FILENAME].csv -t csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy can configure Item Loaders & Input and Output processors for scraped data\n",
    "The input processor functionallity is used to realize changes over raw scraped data.\n",
    "This elements are declared over the 'items.py' file. The input processors are \n",
    "structured as the following code shows\n",
    "\n",
    "'from scrapy.loader.processors import MapCompose\n",
    "\n",
    "def full_links(company_symbol_link):\n",
    "    url = 'https://finance.yahoo.com/trending-tickers'\n",
    "    return url + company_symbol_link\n",
    "\n",
    "class DetailLoader(scrapy.Item):\n",
    "    company_symbol = scrapy.Field()\n",
    "    company_name = scrapy.Field()\n",
    "    company_price = scrapy.Field()\n",
    "    company_link = scrapy.Field(input_processor = MapCompose(full_links))'\n",
    "\n",
    "An item loader object is an object that facilitates the population of information \n",
    "over an 'scrapy.Item' object. This functionallity is managed by instanciating\n",
    "an item loader object and then providing the item intance and the selector element.\n",
    "Once the item loader object is instaciated there has to be executed the \n",
    "'add_xpath()' method. This method must recieve the specific XPath derived from the\n",
    "current selector and also the field name of the 'scrapy.Item' meant to be populated\n",
    "\n",
    "For didactic porpuses there will be created a 'details4.py' spider.\n",
    "In this file there will stored the following code\n",
    "\n",
    "import scrapy\n",
    "from simplespider.items import DetailLoader\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "'class ItemData(scrapy.Spider):\n",
    "    name = 'detailloadersio'\n",
    "    allowed_domains = ['finance.yahoo.com']\n",
    "    start_urls = ['https://finance.yahoo.com/trending-tickers']\n",
    "    def parse(self,response):\n",
    "        # XPaths\n",
    "        xpath_a = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr'\n",
    "        # XPath parse assignment\n",
    "        stock_info = response.xpath(xpath_a).extract()\n",
    "        # Loop assignment\n",
    "        for info in stock_info:\n",
    "            detail_loader = ItemLoader(item = DetailLoader(), selector = info)\n",
    "            detail_loader.add_xpath('company_symbol', 'td[1]/text()')\n",
    "            detail_loader.add_xpath('company_name', 'td[2]/text()')\n",
    "            detail_loader.add_xpath('company_price', 'td[3]/fin-streamer/text()')\n",
    "            detail_loader.add_xpath('company_link', 'td[1]/a/@href')\n",
    "            yield detail_loader.load_item()'\n",
    "\n",
    "Note that by default this method employed appends each XPath element string to a \n",
    "list for each field, and then, it yields the result rather than just adding a \n",
    "string for each field. This is because in some cases it is required to store\n",
    "multiple elements to a field at once (for instance, to list a set of hastags in \n",
    "an image). To provide a plain string there has to be an after process for each\n",
    "scraped data. For that there will be used an output processor. This \n",
    "functionallity is employed over a item loader object sttribute where another \n",
    "object gets instantiated.\n",
    "\n",
    "import scrapy\n",
    "from simplespider.items import DetailLoader\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import TakeFirst ######## OUPUT PROCESSOR\n",
    "\n",
    "class ItemData(scrapy.Spider):\n",
    "    name = 'detailloadersio'\n",
    "    allowed_domains = ['finance.yahoo.com']\n",
    "    start_urls = ['https://finance.yahoo.com/trending-tickers']\n",
    "    def parse(self,response):\n",
    "        # XPaths\n",
    "        xpath_a = '//*[@id=\"list-res-table\"]/div[1]/table/tbody/tr'\n",
    "        # XPath parse assignment\n",
    "        stock_info = response.xpath(xpath_a)\n",
    "        # Loop assignment\n",
    "        for info in stock_info:\n",
    "            detail_loader = ItemLoader(item = DetailLoader(), selector = info)\n",
    "            detail_loader.default_output_processor = TakeFirst() ######## OUPUT PROCESSOR\n",
    "            detail_loader.add_xpath('company_symbol', 'td[1]/a/text()')\n",
    "            detail_loader.add_xpath('company_name', 'td[2]/text()')\n",
    "            detail_loader.add_xpath('company_price', 'td[3]/fin-streamer/text()')\n",
    "            detail_loader.add_xpath('company_link', 'td[1]/a/@href')\n",
    "            yield detail_loader.load_item()\n",
    "\n",
    "Once again there can be executed the '-o' flag over the crawling process to create\n",
    "a CSV file and store gathered data. There can be used the following command\n",
    "\n",
    "scrapy crawl detailloadersio -o itemsdata1.csv -t csv   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be used pipelines to scrape data from the web. This functionallity is\n",
    "included by default over every Scrapy project but it can be setted to execute \n",
    "data processing in every required maner and this can be to filter, validate\n",
    "or manipulate data in any useful manner. The pipeline will modify a price return\n",
    "if it match some criteria.To do so the 'pipelines.py' file will be modified to \n",
    "include the following code\n",
    "\n",
    "'class FilterData(object):\n",
    "    def process_item100 (self, item, spider):\n",
    "        try:\n",
    "            price = float (item['company_price'])\n",
    "            if price > 100:\n",
    "                item['company_price'] = '>100'\n",
    "        except:\n",
    "            pass\n",
    "        return item\n",
    "        \n",
    "    def process_item50 (self, item, spider):\n",
    "        try:\n",
    "            price = float (item['company_price'])\n",
    "            if price < 50:\n",
    "                item['company_price'] = '<50'\n",
    "        except:\n",
    "            pass\n",
    "        return item'\n",
    "\n",
    "Then to make this pipeline available there has to be modified the 'settings.py' file\n",
    "to include the classess in a dictionary called 'ITEM_PIPELINES'. Such dictionary\n",
    "will have as keywords the 'project.module.class' and as a value a priority index\n",
    "to hierarchically execute each pipeline. Note that for this example there was created \n",
    "a copy of the 'details4.py' called 'details5.py' called pipeline\n",
    "to execute there will be provided the following command\n",
    "\n",
    "'scrapy crawl pipeline -o itemsdata2.csv -t csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('BE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0913f2e2c06d0771fd9d7fa954e689c47dbb4161d751da8788b51a616cd1eddd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
